{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense, Flatten, BatchNormalization, Conv2D, MaxPool2D, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "background = None\n",
    "accumulated_weight = 0.5\n",
    "#Creating the dimensions for the ROI...\n",
    "ROI_top = 100\n",
    "ROI_bottom = 300\n",
    "ROI_right = 150\n",
    "ROI_left = 350\n",
    "def cal_accum_avg(frame, accumulated_weight):\n",
    "    global background\n",
    "    \n",
    "    if background is None:\n",
    "        background = frame.copy().astype(\"float\")\n",
    "        return None\n",
    "    cv2.accumulateWeighted(frame, background, accumulated_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_hand(frame, threshold=25):\n",
    "    global background\n",
    "    \n",
    "    diff = cv2.absdiff(background.astype(\"uint8\"), frame)\n",
    "\n",
    "    _ , thresholded = cv2.threshold(diff, threshold,255,cv2.THRESH_BINARY)\n",
    "\n",
    "    # Grab the external contours for the image\n",
    "    image, contours, hierarchy = cv2.findContours(thresholded.copy(),\n",
    "    cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    if len(contours) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        \n",
    "        hand_segment_max_cont = max(contours, key=cv2.contourArea)\n",
    "        \n",
    "        return (thresholded, hand_segment_max_cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam = cv2.VideoCapture(0)\n",
    "\n",
    "num_frames = 0\n",
    "element = 3\n",
    "num_imgs_taken = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cam.read()\n",
    "\n",
    "    # flipping the frame to prevent inverted image of captured frame...\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    frame_copy = frame.copy()\n",
    "\n",
    "    roi = frame[ROI_top:ROI_bottom, ROI_right:ROI_left]\n",
    "\n",
    "    gray_frame = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "    gray_frame = cv2.GaussianBlur(gray_frame, (9, 9), 0)\n",
    "\n",
    "    if num_frames < 60:\n",
    "        cal_accum_avg(gray_frame, accumulated_weight)\n",
    "        if num_frames <= 59:\n",
    "            \n",
    "            cv2.putText(frame_copy, \"FETCHING BACKGROUND...PLEASE WAIT\",\n",
    "(80, 400), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0,0,255), 2)\n",
    "            \n",
    "    #Time to configure the hand specifically into the ROI...\n",
    "    elif num_frames <= 300: \n",
    "\n",
    "        hand = segment_hand(gray_frame)\n",
    "        \n",
    "        cv2.putText(frame_copy, \"Adjust hand...Gesture for\" +\n",
    "  str(element), (200, 400), cv2.FONT_HERSHEY_SIMPLEX, 1,\n",
    "  (0,0,255),2)\n",
    "        \n",
    "        # Checking if the hand is actually detected by counting the number of contours detected...\n",
    "        if hand is not None:\n",
    "            \n",
    "            thresholded, hand_segment = hand\n",
    "\n",
    "            # Draw contours around hand segment\n",
    "            cv2.drawContours(frame_copy, [hand_segment + (ROI_right,\n",
    "            ROI_top)], -1, (255, 0, 0),1)\n",
    "            \n",
    "            cv2.putText(frame_copy, str(num_frames)+\"For\" + str(element),\n",
    "            (70, 45), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "\n",
    "            # Also display the thresholded image\n",
    "            cv2.imshow(\"Thresholded Hand Image\", thresholded)\n",
    "    \n",
    "    else: \n",
    "        \n",
    "        # Segmenting the hand region...\n",
    "        hand = segment_hand(gray_frame)\n",
    "        \n",
    "        # Checking if we are able to detect the hand...\n",
    "        if hand is not None:\n",
    "            \n",
    "            # unpack the thresholded img and the max_contour...\n",
    "            thresholded, hand_segment = hand\n",
    "\n",
    "            # Drawing contours around hand segment\n",
    "            cv2.drawContours(frame_copy, [hand_segment + (ROI_right,\n",
    "            ROI_top)], -1, (255, 0, 0),1)\n",
    "            \n",
    "            cv2.putText(frame_copy, str(num_frames), (70, 45),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "            \n",
    "            cv2.putText(frame_copy, str(num_imgs_taken) + 'images' +\"For\"\n",
    "      + str(element), (200, 400), cv2.FONT_HERSHEY_SIMPLEX, 1,\n",
    "      (0,0,255), 2)\n",
    "            \n",
    "            # Displaying the thresholded image\n",
    "            cv2.imshow(\"Thresholded Hand Image\", thresholded)\n",
    "            if num_imgs_taken <= 900:\n",
    "                cv2.imwrite(r\"C:\\Users\\Ashwath\\Desktop\\try\\sign\\gesture\\train\\9\"+ \"\\\\\" + str(num_imgs_taken) + '.jpg', thresholded)\n",
    "                \n",
    "                 \n",
    "            else:\n",
    "                break\n",
    "            num_imgs_taken +=1\n",
    "        else:\n",
    "            cv2.putText(frame_copy, 'No hand detected...', (200, 400),\n",
    " cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Drawing ROI on frame copy\n",
    "    cv2.rectangle(frame_copy, (ROI_left, ROI_top), (ROI_right,ROI_bottom), (255,128,0), 3)\n",
    "    \n",
    "    cv2.putText(frame_copy, \"DataFlair hand sign recognition_ _ _\", (10, 20), cv2.FONT_ITALIC, 0.5, (51,255,51), 1)\n",
    "    \n",
    "    # increment the number of frames for tracking\n",
    "    num_frames += 1\n",
    "\n",
    "    # Display the frame with segmented hand\n",
    "    cv2.imshow(\"Sign Detection\", frame_copy)\n",
    "\n",
    "    # Closing windows with Esc key...(any other key with ord can be used too.)\n",
    "    k = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "# Releasing the camera & destroying all the windows...\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cam.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 168 images belonging to 10 classes.\n",
      "Found 3010 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "train_path = r'C:\\Users\\Ashwath\\Desktop\\try\\sign\\gesture\\test'\n",
    "test_path = r'C:\\Users\\Ashwath\\Desktop\\try\\sign\\gesture\\train'\n",
    "train_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input).flow_from_directory(directory=train_path, target_size=(64,64), class_mode='categorical', batch_size=10,shuffle=True)\n",
    "test_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input).flow_from_directory(directory=test_path, target_size=(64,64), class_mode='categorical', batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2160x1440 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 64, 64, 3)\n",
      "[[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "imgs, labels = next(train_batches)\n",
    "\n",
    "#Plotting the images...\n",
    "def plotImages(images_arr):\n",
    "    fig, axes = plt.subplots(1, 10, figsize=(30,20))\n",
    "    axes = axes.flatten()\n",
    "    for img, ax in zip( images_arr, axes):\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plotImages(imgs)\n",
    "print(imgs.shape)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"rom tensorflow.keras.applications import VGG16\\n\\n# include top should be False to remove the softmax layer\\npretrained_model = VGG16(include_top=False, weights='imagenet')\\npretrained_model.summary()\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''rom tensorflow.keras.applications import VGG16\n",
    "\n",
    "# include top should be False to remove the softmax layer\n",
    "pretrained_model = VGG16(include_top=False, weights='imagenet')\n",
    "pretrained_model.summary()\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "# extract train and val features\n",
    "vgg_features_train = pretrained_model.predict(train_batches)\n",
    "vgg_features_val = pretrained_model.predict(test_batches)\n",
    "train_target = to_categorical(train_batches.labels)\n",
    "val_target = to_categorical(test_batches.labels)\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(64,64,3)))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(10, activation='softmax'))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "#convolution layers\n",
    "model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(64,64,3)))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding = 'same'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding = 'valid'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "model.add(Flatten())\n",
    "#Fully connected layer\n",
    "model.add(Dense(64,activation =\"relu\"))\n",
    "model.add(Dense(128,activation =\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(128,activation =\"relu\"))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(10,activation =\"softmax\"))\n",
    "\n",
    "\n",
    "# compile the model\n",
    "#model.compile(optimizer='adam', metrics=['accuracy'], loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_21 (Conv2D)           (None, 62, 62, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling (None, 31, 31, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 31, 31, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling (None, 15, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 13, 13, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_23 (MaxPooling (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 64)                294976    \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 414,346\n",
      "Trainable params: 414,346\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=0.0001)\n",
    "#early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer=SGD(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=0.0005)\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "17/17 [==============================] - 60s 4s/step - loss: 8.1418 - accuracy: 0.2024 - val_loss: 2.1305 - val_accuracy: 0.3130\n",
      "Epoch 2/20\n",
      "17/17 [==============================] - 7s 447ms/step - loss: 1.8521 - accuracy: 0.3214 - val_loss: 1.8384 - val_accuracy: 0.4455\n",
      "Epoch 3/20\n",
      "17/17 [==============================] - 7s 448ms/step - loss: 1.4159 - accuracy: 0.5119 - val_loss: 1.4218 - val_accuracy: 0.6033\n",
      "Epoch 4/20\n",
      "17/17 [==============================] - 7s 448ms/step - loss: 1.1062 - accuracy: 0.5952 - val_loss: 1.2768 - val_accuracy: 0.6774\n",
      "Epoch 5/20\n",
      "17/17 [==============================] - 7s 445ms/step - loss: 0.9217 - accuracy: 0.6905 - val_loss: 1.1969 - val_accuracy: 0.6528\n",
      "Epoch 6/20\n",
      "17/17 [==============================] - 7s 445ms/step - loss: 0.6755 - accuracy: 0.7798 - val_loss: 1.1725 - val_accuracy: 0.6831\n",
      "Epoch 7/20\n",
      "17/17 [==============================] - 7s 457ms/step - loss: 0.5867 - accuracy: 0.7679 - val_loss: 0.9677 - val_accuracy: 0.7150\n",
      "Epoch 8/20\n",
      "17/17 [==============================] - 8s 491ms/step - loss: 0.4551 - accuracy: 0.8690 - val_loss: 1.0725 - val_accuracy: 0.6804\n",
      "Epoch 9/20\n",
      "17/17 [==============================] - 8s 470ms/step - loss: 0.4408 - accuracy: 0.9048 - val_loss: 0.8246 - val_accuracy: 0.7894\n",
      "Epoch 10/20\n",
      "17/17 [==============================] - 7s 450ms/step - loss: 0.2589 - accuracy: 0.9226 - val_loss: 1.0761 - val_accuracy: 0.7575\n",
      "Epoch 11/20\n",
      "17/17 [==============================] - 7s 441ms/step - loss: 0.3144 - accuracy: 0.9167 - val_loss: 0.9796 - val_accuracy: 0.7807\n",
      "Epoch 12/20\n",
      "17/17 [==============================] - 7s 449ms/step - loss: 0.2135 - accuracy: 0.9345 - val_loss: 1.0837 - val_accuracy: 0.7518\n",
      "Epoch 13/20\n",
      "17/17 [==============================] - 7s 443ms/step - loss: 0.1073 - accuracy: 0.9643 - val_loss: 1.0679 - val_accuracy: 0.7518\n",
      "Epoch 14/20\n",
      "17/17 [==============================] - 7s 451ms/step - loss: 0.1115 - accuracy: 0.9762 - val_loss: 1.2574 - val_accuracy: 0.7625\n",
      "Epoch 15/20\n",
      "17/17 [==============================] - 7s 445ms/step - loss: 0.2100 - accuracy: 0.9345 - val_loss: 1.1748 - val_accuracy: 0.7585\n",
      "Epoch 16/20\n",
      "17/17 [==============================] - 7s 450ms/step - loss: 0.1600 - accuracy: 0.9345 - val_loss: 1.1662 - val_accuracy: 0.7551\n",
      "Epoch 17/20\n",
      "17/17 [==============================] - 7s 438ms/step - loss: 0.1148 - accuracy: 0.9762 - val_loss: 1.1953 - val_accuracy: 0.7568\n",
      "Epoch 18/20\n",
      "17/17 [==============================] - 7s 439ms/step - loss: 0.0875 - accuracy: 0.9643 - val_loss: 1.2399 - val_accuracy: 0.7698\n",
      "Epoch 19/20\n",
      "17/17 [==============================] - 7s 444ms/step - loss: 0.1319 - accuracy: 0.9583 - val_loss: 1.0056 - val_accuracy: 0.7738\n",
      "Epoch 20/20\n",
      "17/17 [==============================] - 7s 440ms/step - loss: 0.1384 - accuracy: 0.9583 - val_loss: 1.2291 - val_accuracy: 0.7482\n"
     ]
    }
   ],
   "source": [
    "#history2 = model.fit(train_batches, epochs=30, callbacks=[reduce_lr, early_stop],  validation_data = test_batches)\n",
    "history2 = model.fit(train_batches, epochs=20, validation_data = test_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss of 0.840304970741272; accuracy of 89.99999761581421%\n"
     ]
    }
   ],
   "source": [
    "# For getting next batch of testing imgs...\n",
    "imgs, labels = next(test_batches) \n",
    "\n",
    "scores = model.evaluate(imgs, labels, verbose=0)\n",
    "print(f'{model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "\n",
    "\n",
    "#Once the model is fitted we save the model using model.save()  function.\n",
    "\n",
    "\n",
    "model.save('30sgd.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000029DD75EE3A8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions on a small set of test data--\n",
      "\n",
      "One   Two   Five   Eight   Two   Five   Nine   One   Five   Seven   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2160x1440 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual labels\n",
      "One   Six   Five   Eight   Two   Five   Nine   One   Five   Seven   "
     ]
    }
   ],
   "source": [
    "word_dict = {0:'Zero',1:'One',2:'Two',3:'Three',4:'Four',5:'Five',6:'Six',7:'Seven',8:'Eight',9:'Nine'}\n",
    "\n",
    "predictions = model.predict(imgs, verbose=0)\n",
    "print(\"predictions on a small set of test data--\")\n",
    "print(\"\")\n",
    "for ind, i in enumerate(predictions):\n",
    "    print(word_dict[np.argmax(i)], end='   ')\n",
    "\n",
    "plotImages(imgs)\n",
    "print('Actual labels')\n",
    "for i in labels:\n",
    "    print(word_dict[np.argmax(i)], end='   ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Realtime Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(r\"C:\\Users\\Ashwath\\Desktop\\try\\sign\\best_model_dataflair3.h5\")\n",
    "\n",
    "background = None\n",
    "accumulated_weight = 0.5\n",
    "\n",
    "ROI_top = 100\n",
    "ROI_bottom = 300\n",
    "ROI_right = 150\n",
    "ROI_left = 350"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_accum_avg(frame, accumulated_weight):\n",
    "\n",
    "    global background\n",
    "    \n",
    "    if background is None:\n",
    "        background = frame.copy().astype(\"float\")\n",
    "        return None\n",
    "\n",
    "    cv2.accumulateWeighted(frame, background, accumulated_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_hand(frame, threshold=25):\n",
    "    global background\n",
    "    \n",
    "    diff = cv2.absdiff(background.astype(\"uint8\"), frame)\n",
    "\n",
    "    \n",
    "    _ , thresholded = cv2.threshold(diff, threshold, 255,cv2.THRESH_BINARY)\n",
    "    \n",
    "     #Fetching contours in the frame (These contours can be of hand or any other object in foreground) …\n",
    "\n",
    "    image, contours, hierarchy = cv2.findContours(thresholded.copy(), cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # If length of contours list = 0, means we didn't get any contours...\n",
    "    if len(contours) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        # The largest external contour should be the hand \n",
    "        hand_segment_max_cont = max(contours, key=cv2.contourArea)\n",
    "        \n",
    "        # Returning the hand segment(max contour) and the thresholded image of hand...\n",
    "        return (thresholded, hand_segment_max_cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam = cv2.VideoCapture(0)\n",
    "num_frames =0\n",
    "while True:\n",
    "    ret, frame = cam.read()\n",
    "\n",
    "    # flipping the frame to prevent inverted image of captured frame...\n",
    "    \n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    frame_copy = frame.copy()\n",
    "\n",
    "    # ROI from the frame\n",
    "    roi = frame[ROI_top:ROI_bottom, ROI_right:ROI_left]\n",
    "\n",
    "    gray_frame = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "    gray_frame = cv2.GaussianBlur(gray_frame, (9, 9), 0)\n",
    "\n",
    "\n",
    "    if num_frames < 70:\n",
    "        \n",
    "        cal_accum_avg(gray_frame, accumulated_weight)\n",
    "        \n",
    "        cv2.putText(frame_copy, \"FETCHING BACKGROUND...PLEASE WAIT\",(80, 400), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0,0,255), 2)\n",
    "    \n",
    "    else: \n",
    "        # segmenting the hand region\n",
    "        hand = segment_hand(gray_frame)\n",
    "        \n",
    "        # Checking if we are able to detect the hand...\n",
    "        if hand is not None:\n",
    "            \n",
    "            thresholded, hand_segment = hand\n",
    "\n",
    "            # Drawing contours around hand segment\n",
    "            cv2.drawContours(frame_copy, [hand_segment + (ROI_right,ROI_top)], -1, (255, 0, 0),1)\n",
    "            \n",
    "            cv2.imshow(\"Thesholded Hand Image\", thresholded)\n",
    "            \n",
    "            thresholded = cv2.resize(thresholded, (64, 64))\n",
    "            thresholded = cv2.cvtColor(thresholded,\n",
    " cv2.COLOR_GRAY2RGB)\n",
    "            thresholded = np.reshape(thresholded,\n",
    "(1,thresholded.shape[0],thresholded.shape[1],3))\n",
    "            \n",
    "            pred = model.predict(thresholded)\n",
    "            cv2.putText(frame_copy, word_dict[np.argmax(pred)],\n",
    "(170, 45), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "            \n",
    "    # Draw ROI on frame_copy\n",
    "    cv2.rectangle(frame_copy, (ROI_left, ROI_top), (ROI_right,\n",
    "    ROI_bottom), (255,128,0), 3)\n",
    "\n",
    "    # incrementing the number of frames for tracking\n",
    "    num_frames += 1\n",
    "\n",
    "    # Display the frame with segmented hand\n",
    "    #cv2.putText(frame_copy, \"DataFlair hand sign recognition_ _ _\",\n",
    "    #(10, 20), cv2.FONT_ITALIC, 0.5, (51,255,51), 1)\n",
    "    cv2.imshow(\"Sign Detection\", frame_copy)\n",
    "\n",
    "\n",
    "    # Close windows with Esc\n",
    "    k = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "# Release the camera and destroy all the windows\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
